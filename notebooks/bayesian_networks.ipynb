{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian Networks",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEObLQuEmbRv"
      },
      "source": [
        "# Example Cognitive Vision System (CVS)\r\n",
        "\r\n",
        "In this tutorial, we present an example CVS that uses neural networks and bayesian networks for classification and scene understanding in videos. \r\n",
        "* The neural networks will classify each frame in a video. \r\n",
        "* The bayesian network will use scene understanding to improve the probabilistic inference of the neural network.\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "See the notebook that uses cogvis for image classification before diving into this one. There are details in the classification notebook that are ommitted here for brevity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GWxG1jTrLTO"
      },
      "source": [
        "## Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deWmPNXi_sBL",
        "outputId": "4b32abce-6199-4fb6-c6c0-bfe9bc6025bc"
      },
      "source": [
        "!pip install av"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting av\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/b7/4b1095af7f8e87c0f54fc0a3de9472d09583eaf2e904a60f0817819fff11/av-8.0.3-cp36-cp36m-manylinux2010_x86_64.whl (37.2MB)\n",
            "\u001b[K     |████████████████████████████████| 37.2MB 1.5MB/s \n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-8.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBCbXLkj0Agl",
        "outputId": "06d9a5aa-c2d1-467b-f941-7498b69ae9a4"
      },
      "source": [
        "!pip install cogvis-0.0.1-py3-none-any.whl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./cogvis-0.0.1-py3-none-any.whl\n",
            "Installing collected packages: cogvis\n",
            "Successfully installed cogvis-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzNJ3xTwdUZ2"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "import av\r\n",
        "import pandas as pd\r\n",
        "import pickle\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from PIL import Image\r\n",
        "from scipy import special\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "# and most importantly...\r\n",
        "from cogvis.classification import nn_classification"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_2Uw0n51YO2"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjw5G3ogoxju"
      },
      "source": [
        "## Dataset\r\n",
        "\r\n",
        "We are using a subset of the CIFAR-100 datatset in this application. There are ten classes with each class falling into one of two categories. The classes are bed, chair, couch, table, wardrobe, forest, mountain, plain, sea and cloud. The classes are either found in outdoor scenery or indoor scenery. This separation of classes allows us to incorporate scene understanding into our application using bayesian networks.  \r\n",
        "\r\n",
        "There are only 500 training images for each class and each image is only 32 x 32 pixels. So, we are not expecting a terribly accurate model. All we need is a reasonable model that we can improve using Bayesian networks.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "The images from CIFAR-100 are stored in HDF5 files. The code that produces these files is included below. Before you run any of this code, you must download and `tar -xvzf` the relevant files from this [website](https://www.cs.toronto.edu/~kriz/cifar.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0uf93xUrZI3"
      },
      "source": [
        "import h5py\r\n",
        "\r\n",
        "def unpickle(file):\r\n",
        "    with open(file, 'rb') as fo:\r\n",
        "        data_dict = pickle.load(fo, encoding='latin1')\r\n",
        "    return data_dict\r\n",
        "\r\n",
        "\r\n",
        "def save_images(file_path, meta_path, save_path, target_fine_labels):\r\n",
        "    meta = unpickle(meta_path)\r\n",
        "    data = unpickle(file_path)\r\n",
        "    fine_label_names = meta['fine_label_names']\r\n",
        "\r\n",
        "    idx_to_label = {idx: l for idx, l in enumerate(fine_label_names)}\r\n",
        "    label_idxs = data['fine_labels']\r\n",
        "    imgs = data['data']\r\n",
        "\r\n",
        "    imgs = imgs.reshape(len(imgs),3,32,32).transpose(0,2,3,1) \r\n",
        "    img_dict = {label: [] for label in target_fine_labels}\r\n",
        "\r\n",
        "    for label_idx, img in zip(label_idxs, imgs):\r\n",
        "        label = idx_to_label[label_idx]\r\n",
        "        if label in target_fine_labels:\r\n",
        "            img_dict[label].append(img)\r\n",
        "\r\n",
        "    f = h5py.File(save_path, 'w')\r\n",
        "    for label, imgs in img_dict.items():\r\n",
        "        imgs = np.array(imgs)\r\n",
        "        f.create_dataset(\r\n",
        "                label, \r\n",
        "                np.shape(imgs), \r\n",
        "                h5py.h5t.STD_U8BE, # 8-bit unsigned integer\r\n",
        "                data=imgs,\r\n",
        "                )\r\n",
        "    f.close()\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    meta_path = 'cifar-100-python/meta'\r\n",
        "    train_path = 'cifar-100-python/train'\r\n",
        "    val_path = 'cifar-100-python/test'\r\n",
        "    target_labels = ['cloud', 'forest', 'mountain', 'plain', 'sea', 'bed', \r\n",
        "            'chair', 'couch', 'table', 'wardrobe']\r\n",
        "    save_images(train_path, meta_path, 'train.hdf5', target_labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VGTJJWN2Pjj"
      },
      "source": [
        "## Data loading and transforms\r\n",
        "We perform a couple of complex transforms on the training data to improve the robustness of the resulting model. We only perform the necessary transforms on our validation data. We normalise the image using the mean and standard deviation of each channel for each image in our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9bCagBmdc6K"
      },
      "source": [
        "train_path = '/content/drive/My Drive/cifar_data_hdf5/train.hdf5'\r\n",
        "val_path = '/content/drive/My Drive/cifar_data_hdf5/val.hdf5'\r\n",
        "#train_path = '/content/drive/My Drive/cifar_data/train/'\r\n",
        "#val_path = '/content/drive/My Drive/cifar_data/val/'\r\n",
        "\r\n",
        "transform = nn_classification.data_transform(ToTensor=())\r\n",
        "_, loader = nn_classification.data_loader(train_path, 1000, transform=transform)\r\n",
        "data, _ = next(iter(loader))\r\n",
        "means = [data[:,c].mean().tolist() for c in range(len(data[0]))] \r\n",
        "stds = [data[:,c].std().tolist() for c in range(len(data[0]))]\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "train_transform = nn_classification.data_transform(\r\n",
        "    ColorJitter=(),\r\n",
        "#    RandomHorizontalFlip=(), # only works with PIL images not HDF5\r\n",
        "#    RandomPerspective=(),    # only works with PIL images not HDf5\r\n",
        "    ToTensor=(),\r\n",
        "    Normalize=(means, stds),\r\n",
        ")\r\n",
        "\r\n",
        "val_transform = nn_classification.data_transform(\r\n",
        "    ToTensor=(),\r\n",
        "    Normalize=(means, stds),\r\n",
        ") \r\n",
        "\r\n",
        "train_size, train_loader = nn_classification.data_loader(train_path, batch_size, \r\n",
        "                                                      transform=train_transform)\r\n",
        "\r\n",
        "val_size, val_loader = nn_classification.data_loader(val_path, batch_size, \r\n",
        "                                                     transform=val_transform)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvijcCfcE51k"
      },
      "source": [
        "## Training/Validation\r\n",
        "\r\n",
        "We are using an existing neural network architecture, which is modified to produce the number of desired classes. The returned model has the weights that produced the highest validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyrbIk4TK4p-",
        "outputId": "117f9fac-c915-4bc6-cd59-19de861f95b5"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "nn_model, params = nn_classification.existing('resnet18', 10)\r\n",
        "optimizer = optim.SGD(params, lr=0.001, momentum=0.9)\r\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\r\n",
        "\r\n",
        "nn_model = nn_classification.train(\r\n",
        "        nn_model, \r\n",
        "        train_loader, val_loader, \r\n",
        "        train_size, val_size, \r\n",
        "        criterion, \r\n",
        "        optimizer,\r\n",
        "        exp_lr_scheduler, \r\n",
        "        num_epochs=10,\r\n",
        "        )"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/9\n",
            "----------\n",
            "Train Loss: 1.7210 Acc: 0.3942\n",
            "Val. Loss: 1.4807 Acc: 0.4850\n",
            "\n",
            "Epoch: 1/9\n",
            "----------\n",
            "Train Loss: 1.2944 Acc: 0.5402\n",
            "Val. Loss: 1.3385 Acc: 0.5280\n",
            "\n",
            "Epoch: 2/9\n",
            "----------\n",
            "Train Loss: 1.0915 Acc: 0.6124\n",
            "Val. Loss: 1.2949 Acc: 0.5500\n",
            "\n",
            "Epoch: 3/9\n",
            "----------\n",
            "Train Loss: 0.9091 Acc: 0.6772\n",
            "Val. Loss: 1.2796 Acc: 0.5740\n",
            "\n",
            "Epoch: 4/9\n",
            "----------\n",
            "Train Loss: 0.7751 Acc: 0.7262\n",
            "Val. Loss: 1.2608 Acc: 0.5830\n",
            "\n",
            "Epoch: 5/9\n",
            "----------\n",
            "Train Loss: 0.6204 Acc: 0.7854\n",
            "Val. Loss: 1.2185 Acc: 0.5970\n",
            "\n",
            "Epoch: 6/9\n",
            "----------\n",
            "Train Loss: 0.4805 Acc: 0.8344\n",
            "Val. Loss: 1.3942 Acc: 0.6110\n",
            "\n",
            "Epoch: 7/9\n",
            "----------\n",
            "Train Loss: 0.2797 Acc: 0.9168\n",
            "Val. Loss: 1.2537 Acc: 0.6330\n",
            "\n",
            "Epoch: 8/9\n",
            "----------\n",
            "Train Loss: 0.2032 Acc: 0.9454\n",
            "Val. Loss: 1.2329 Acc: 0.6440\n",
            "\n",
            "Epoch: 9/9\n",
            "----------\n",
            "Train Loss: 0.1835 Acc: 0.9530\n",
            "Val. Loss: 1.2324 Acc: 0.6420\n",
            "\n",
            "Best Val. Acc: 0.644000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zilIreF6WQt4"
      },
      "source": [
        "## Prediction\r\n",
        "\r\n",
        "We are now going to get neural network predictions and associated probabilities for two sample videos. The first video contains indoor scenery and the second contains outdoor scenery. Since we know the scenery of each video, we can measure the accuracy of the model by looking at erroneous probabilities assigned to objects not in the given environment (indoor vs outdoor)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2U2wozIAOdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cc29c8-02d0-4065-b445-f5bdd55d6411"
      },
      "source": [
        "def vid_to_arrays(file_path):\r\n",
        "  arrays = []\r\n",
        "  container = av.open(file_path)\r\n",
        "  for packet in container.demux():\r\n",
        "    for frame in packet.decode():            \r\n",
        "      if type(frame) == av.video.frame.VideoFrame:\r\n",
        "        img = frame.to_image()\r\n",
        "        arr = np.asarray(img)\r\n",
        "        arrays.append(arr)\r\n",
        "  return arrays\r\n",
        "\r\n",
        "\r\n",
        "def arrays_to_inputs(arrays, transform):\r\n",
        "  inputs = []\r\n",
        "  for arr in arrays:\r\n",
        "    img = Image.fromarray(arr)\r\n",
        "    input = transform(img)\r\n",
        "    inputs.append(input.numpy())\r\n",
        "  return torch.Tensor(np.array(inputs))\r\n",
        "\r\n",
        "\r\n",
        "def accuracy(preds, probs, env_target, class_to_idx, env_to_class):\r\n",
        "  target_class_labels = env_to_class[env_target]\r\n",
        "  target_class_idxs = [class_to_idx[cls] for cls in target_class_labels]\r\n",
        "  num_correct = 0\r\n",
        "  for prob in probs:\r\n",
        "    target_probs = [prob[idx] for idx in target_class_idxs]\r\n",
        "    if sum(target_probs) > 0.5:\r\n",
        "      num_correct += 1\r\n",
        "  return num_correct/len(probs)\r\n",
        "\r\n",
        "\r\n",
        "transform = nn_classification.data_transform(\r\n",
        "    Resize=(40,),\r\n",
        "    CenterCrop=(32,), # same size as training images\r\n",
        "    ToTensor=(),\r\n",
        "    Normalize=(means, stds),\r\n",
        ")\r\n",
        "\r\n",
        "vid1 = '/content/drive/My Drive/CVS_videos/indoor.mp4'\r\n",
        "vid2 = '/content/drive/My Drive/CVS_videos/outdoor.mp4'\r\n",
        "\r\n",
        "arrays1 = vid_to_arrays(vid1)\r\n",
        "arrays2 = vid_to_arrays(vid2)\r\n",
        "\r\n",
        "inputs1 = arrays_to_inputs(arrays1, transform)\r\n",
        "inputs2 = arrays_to_inputs(arrays2, transform)\r\n",
        "\r\n",
        "class_to_idx = train_loader.dataset.class_to_idx\r\n",
        "env_to_class = {\r\n",
        "      'in': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\r\n",
        "      'out': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\r\n",
        "  }\r\n",
        "\r\n",
        "preds1, probs1 = nn_classification.predict(nn_model, inputs1)\r\n",
        "preds2, probs2 = nn_classification.predict(nn_model, inputs2)\r\n",
        "\r\n",
        "acc1 = accuracy(preds1, probs1, 'in', class_to_idx, env_to_class)\r\n",
        "acc2 = accuracy(preds2, probs2, 'out', class_to_idx, env_to_class)\r\n",
        "\r\n",
        "print(f'Indoor accuracy: {acc1*100:.2f}')\r\n",
        "print(f'Outdoor accuracy: {acc2*100:.2f}')\r\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indoor accuracy: 73.22\n",
            "Outdoor accuracy: 80.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wf41ZMt4qSa"
      },
      "source": [
        "## Bayesian network\r\n",
        "\r\n",
        "This section will serve as a ***proof of concept*** for using Bayesian Networks (BN) for scene understanding in videos. The functionality in this section should serve as inspiration rather than the starting point of the cogvis BN component.\r\n",
        "\r\n",
        "An attempt was made to use existing frameworks such as [BayesPy](https://www.bayespy.org/) and [Pomegranate](https://pomegranate.readthedocs.io/en/latest/). These are both small projects, which are fairly narrow in scope (not to mention BayesPy isn't actively maintained). The functionality provided was specific to a select few problems. These problems didn't seem extensible in a general way. I am of the opinion that these frameworks are not suibtable for the BN component of cogvis, although further consideration/revision may still be needed to make a decision.\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "Here, I create a simple BN using given conditional probabilities. Further work includes:\r\n",
        "* d-separation\r\n",
        "* automatic learning\r\n",
        "* plate notation for BN instantiation\r\n",
        "* variable elimination (with optimal ordering)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrjQYhXruuR"
      },
      "source": [
        "# ------------------------------------------------------------------ #\r\n",
        "\r\n",
        "# IMPORTANT: untested, rough idea for how to implement a BN framework\r\n",
        "# TODO: much consideration and improvement\r\n",
        "# NOTE: probably best used as inspiration rather than starting code\r\n",
        "\r\n",
        "# ------------------------------------------------------------------ #\r\n",
        "\r\n",
        "\r\n",
        "class Node():\r\n",
        "  def __init__(self, name, distribution):\r\n",
        "    self.name = name\r\n",
        "    self.distribution = distribution\r\n",
        "    self.parents = []\r\n",
        "  \r\n",
        "  def __eq__(self, other):\r\n",
        "    if type(other) is str:\r\n",
        "      return self.name == other\r\n",
        "    return self.name == other.name\r\n",
        "\r\n",
        "\r\n",
        "class BayesianNetwork():\r\n",
        "  \r\n",
        "  def __init__(self, nodes, edges):\r\n",
        "    for parent, child in edges:\r\n",
        "      child.parents.append(parent)\r\n",
        "    self.nodes = nodes\r\n",
        "  \r\n",
        "  def observe(self, target, dist):\r\n",
        "    idx = self.nodes.index(target)\r\n",
        "    node = self.nodes[idx]\r\n",
        "    node.distribution = dist\r\n",
        "\r\n",
        "  def probability(self, target):\r\n",
        "    # NOTE: currently only works with the implemented example below (2 nodes)\r\n",
        "    # NOTE: That is, one initial distribution and the other a conditional distribution\r\n",
        "    # TODO: generalise\r\n",
        "    node = self.nodes[self.nodes.index(target)]\r\n",
        "    joint_dist = self.joint(node.distribution, node.parents[0].distribution)\r\n",
        "    prob_dist = self.marginal(joint_dist, 0)\r\n",
        "    return prob_dist\r\n",
        "\r\n",
        "  def ancestors(self, node):\r\n",
        "    if not node.parents:\r\n",
        "      return [node]\r\n",
        "    ancestors = []\r\n",
        "    for parent in node.parents:\r\n",
        "      ancestors = ancestors + self.ancestors(parent)\r\n",
        "    return [node] + ancestors\r\n",
        "\r\n",
        "  def joint(self, child_dist, parent_dist):\r\n",
        "    assert(all([i == c for i, c in zip(child_dist.index, parent_dist.columns)]))\r\n",
        "    joint_vars = [(c1, c2) for c1 in child_dist.columns for c2 in parent_dist.columns]\r\n",
        "\r\n",
        "    heading = lambda x: x[0] + ' ' + x[1]\r\n",
        "    columns = list(map(heading, joint_vars))\r\n",
        "    idxs = parent_dist.index\r\n",
        "    joint_dist = pd.DataFrame(columns=columns, index=idxs)\r\n",
        "\r\n",
        "    for c1, c2 in joint_vars:\r\n",
        "      for idx in idxs:\r\n",
        "        prob = child_dist[c1][c2] * parent_dist[c2][idx]\r\n",
        "        col = c1 + ' ' + c2\r\n",
        "        joint_dist[col][idx] = prob\r\n",
        "    return joint_dist\r\n",
        "\r\n",
        "\r\n",
        "  def marginal(self, dist, target_index):\r\n",
        "    marg_cols = list(set([c.split(' ')[target_index] for c in dist.columns]))\r\n",
        "    marg_dist = pd.DataFrame(0, columns=marg_cols, index=dist.index)\r\n",
        "    for col in dist.columns:\r\n",
        "      for idx in dist.index:\r\n",
        "        marg_col = col.split(' ')[target_index]\r\n",
        "        marg_dist.loc[idx, marg_col] += dist[col][idx]\r\n",
        "    return marg_dist\r\n",
        "\r\n",
        "\r\n",
        "def combine_cond(cond1, cond2, dist):\r\n",
        "  # NOTE: currently unused - combines parents P(X|Y0) and P(X|Y1) to P(X|Y0,Y1)\r\n",
        "  # TODO: incorporate into Bayesian Network class\r\n",
        "  # TODO: modify and standardise along with all other BN methods\r\n",
        "  assert(all([n1 == n2 for n1, n2 in zip(cond1.columns, cond2.columns)]))\r\n",
        "  index = [c1 + ' ' + c2 for c1 in cond1.index for c2 in cond2.index]\r\n",
        "  cond = pd.DataFrame(index=index, columns=cond1.columns)\r\n",
        "  for col in cond1.columns:\r\n",
        "    for c1 in cond1.index:\r\n",
        "      for c2 in cond2.index:\r\n",
        "        index = c1 + ' ' + c2\r\n",
        "        cond[col][index] = (cond1[col][c1] * cond2[col][c2]) / dist[col]\r\n",
        "  arr = cond.to_numpy(dtype=float)\r\n",
        "  normalised = special.softmax(arr, axis=1)\r\n",
        "  cond = pd.DataFrame(normalised, index=cond.index, columns=cond.columns)\r\n",
        "  return cond\r\n",
        "\r\n",
        "\r\n",
        "def csv_to_cond(filename):\r\n",
        "  cond = pd.read_csv(filename, header=0, index_col=0)\r\n",
        "  cond.rename(columns=str.strip, index=str.strip)\r\n",
        "  col_names = [c for c in cond.columns]\r\n",
        "  cond[col_names] = cond[col_names].astype(float)\r\n",
        "  return cond"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Itc2kNcQQur6"
      },
      "source": [
        "Let's create a simple BN that uses scene understanding to improve the probabilistic predictions of our model. Each node in the network contains an (initial or conditional) distribution.  \r\n",
        "\r\n",
        "Let's assume we know whether we are outside or inside. That is to say, we observe the state of the environment\r\n",
        "and infer the probability of each object using the given conditional probability table. \r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "Each conditional distribution is represented by a pandas `DataFrame`, where each column heading represents a random variable and each row index represents a conditional variable.  \r\n",
        "* Joint distributions contain multiple variables in the column heading (seperated by a `' '`).\r\n",
        "* Distributions conditioned on multiple variables have these variables as an index (seperated by a `' '`)\r\n",
        "* Initial distributions have the default index.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwl8qoBO7yMb"
      },
      "source": [
        "## Improving inference using a BN (known environment)\r\n",
        "\r\n",
        "* We instantiate a bayesian network that has two nodes, one being an initial distribution of the environment and the other being a conditional distribution of an object given an environment. \r\n",
        "* Next, we assume to know the environment. We insert this evidence using the `observe` method. \r\n",
        "* Finally, we get the probability distribution of the required variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvOpOaySRBOu",
        "outputId": "492f3707-15f0-40c4-9376-afdab09c5e39"
      },
      "source": [
        "env = pd.DataFrame.from_dict({'in': [0.5], 'out': [0.5]}) # P(env)\r\n",
        "obj_env = csv_to_cond('/content/drive/My Drive/bayes_conditionals/obj_env.csv') # P(obj | env)\r\n",
        "\r\n",
        "A = Node('A', env)\r\n",
        "B = Node('B', obj_env)\r\n",
        "nodes = [A, B]\r\n",
        "edges = [(A, B)]\r\n",
        "bn = BayesianNetwork(nodes, edges)\r\n",
        "\r\n",
        "# Let's first assume we are inside\r\n",
        "env = pd.DataFrame.from_dict({'in': [1.0], 'out': [0.0]})\r\n",
        "bn.observe('A', env)\r\n",
        "bn_probs1 = bn.probability('B') # P(obj)\r\n",
        "print(bn_probs1, '\\n')\r\n",
        "\r\n",
        "# Now let's assume we are outside\r\n",
        "env = pd.DataFrame.from_dict({'in': [0.0], 'out': [1.0]})\r\n",
        "bn.observe('A', env)\r\n",
        "bn_probs2 = bn.probability('B') # P(obj)\r\n",
        "print(bn_probs2)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   table  wardrobe   bed  cloud  chair   sea  mountain  plain  forest  couch\n",
            "0   0.24      0.15  0.15   0.02   0.24  0.01      0.02   0.01    0.01   0.15 \n",
            "\n",
            "   table  wardrobe   bed  cloud  chair  sea  mountain  plain  forest  couch\n",
            "0   0.02      0.01  0.01    0.2   0.05  0.2      0.15   0.15     0.2   0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQG6z_T8KKOH"
      },
      "source": [
        "Using these probabilities, we can modify the prediction probabilities of our neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw0KiEJYjk3p"
      },
      "source": [
        "def modify(vid_probs, bn_probs, class_to_idx, env_to_class, target):\r\n",
        "  # TODO: this is janky\r\n",
        "  target_classes = env_to_class[target]\r\n",
        "  other_classes = [c for c in class_to_idx.keys() if c not in target_classes]\r\n",
        "\r\n",
        "  target_idxs = [class_to_idx[c] for c in target_classes]\r\n",
        "  other_idxs = [class_to_idx[c] for c in other_classes]\r\n",
        "\r\n",
        "  bn_cols = sorted(bn_probs.columns, key=lambda c: class_to_idx[c])\r\n",
        "  bn_probs = [bn_probs.loc[0, c] for c in bn_cols]\r\n",
        "\r\n",
        "  modified_probs = pd.DataFrame(columns=vid_probs.columns, index=vid_probs.index)\r\n",
        "  for idx in vid_probs.index:\r\n",
        "    error_prob = sum(vid_probs.iloc[idx, other_idxs])\r\n",
        "    vid_probs.iloc[idx, other_idxs] = 0\r\n",
        "    delta_probs = np.asarray([p * error_prob for p in bn_probs])\r\n",
        "    modified_probs.loc[idx,:] = delta_probs + vid_probs.loc[idx, :].to_numpy()\r\n",
        "  return modified_probs\r\n",
        "\r\n",
        "\r\n",
        "columns = list(range(len(bn_probs1.columns)))\r\n",
        "vid_probs1 = pd.DataFrame(probs1, columns=columns)\r\n",
        "vid_probs2 = pd.DataFrame(probs2, columns=columns)\r\n",
        "\r\n",
        "final_probs1 = modify(vid_probs1, bn_probs1, class_to_idx, env_to_class, 'in') # inside video\r\n",
        "final_probs2 = modify(vid_probs2, bn_probs2, class_to_idx, env_to_class, 'out') # outside video"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b50cK5tLekuQ"
      },
      "source": [
        "Next, we simply plot and save the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4yCidMlUbPZ"
      },
      "source": [
        "import matplotlib.image as mpimg\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "def save_plot(file_path, img, class_to_idx, nn_probs, bn_probs, transform):\r\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\r\n",
        "\r\n",
        "  ax1.imshow(img)\r\n",
        "  ax1.axes.xaxis.set_visible(False)\r\n",
        "  ax1.axes.yaxis.set_visible(False)\r\n",
        "\r\n",
        "  x = [1] * len(nn_probs)\r\n",
        "  y = list(range(1, len(nn_probs)+1))\r\n",
        "\r\n",
        "  for cls_name, idx in class_to_idx.items():\r\n",
        "    label = f'{cls_name}  {nn_probs[idx]*100:.2f}%'\r\n",
        "    ax2.annotate(label, (x[idx] + 1, y[idx]))\r\n",
        "\r\n",
        "  for cls_name, idx in class_to_idx.items():\r\n",
        "    label = f'{cls_name}  {bn_probs[idx]*100:.2f}%'\r\n",
        "    ax3.annotate(label, (x[idx] + 1, y[idx]))\r\n",
        "  \r\n",
        "  s = [1000 * p for p in nn_probs]\r\n",
        "  ax2.set_xlim(0, len(nn_probs) + 1)\r\n",
        "  ax2.set_ylim(0, len(nn_probs) + 1)\r\n",
        "  ax2.scatter(x, y, s)\r\n",
        "  ax2.axes.xaxis.set_visible(False)\r\n",
        "  ax2.set_title('nn')\r\n",
        "\r\n",
        "  s = [1000 * p for p in bn_probs]\r\n",
        "  ax3.set_xlim(0, len(bn_probs) + 1)\r\n",
        "  ax3.set_ylim(0, len(bn_probs) + 1)\r\n",
        "  ax3.scatter(x, y, s)\r\n",
        "  ax3.axes.xaxis.set_visible(False)\r\n",
        "  ax3.set_title('nn with bn')\r\n",
        "\r\n",
        "  plt.savefig(file_path)\r\n",
        "  plt.close(fig)\r\n",
        "\r\n",
        "\r\n",
        "def save_plots(save_path, imgs, class_to_idx, nn_probs, bn_probs, transform):\r\n",
        "  for i, img in enumerate(imgs):\r\n",
        "    img = transform(img)\r\n",
        "    path = f'{save_path}/{i}.png'\r\n",
        "    save_plot(path, img, class_to_idx, nn_probs[i], bn_probs[i], transform)\r\n",
        "\r\n",
        "\r\n",
        "transform = nn_classification.data_transform(\r\n",
        "    Resize=(40,),\r\n",
        "    CenterCrop=(32,),\r\n",
        ")\r\n",
        "\r\n",
        "arrays1 = vid_to_arrays(vid1)\r\n",
        "imgs1 = [Image.fromarray(arr) for arr in arrays1]\r\n",
        "image_folder = '/content/drive/My Drive/CVS_videos/temp_images'\r\n",
        "save_plots(image_folder, imgs1, class_to_idx, probs1, final_probs1.to_numpy(), transform)\r\n",
        "\r\n",
        "# arrays2 = vid_to_arrays(vid2)\r\n",
        "# imgs2 = [Image.fromarray(arr) for arr in arrays2]\r\n",
        "# image_folder = '/content/drive/My Drive/CVS_videos/temp_images'\r\n",
        "# save_plots(image_folder, imgs2, class_to_idx, probs2, final_probs2.to_numpy(), transform)\r\n",
        "\r\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j-k9VmzTo__"
      },
      "source": [
        "import cv2\r\n",
        "import os\r\n",
        "\r\n",
        "def save_vid(image_folder, vid_path, fps):\r\n",
        "  images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\r\n",
        "  frame = cv2.imread(os.path.join(image_folder, images[0]))\r\n",
        "  height, width, layers = frame.shape\r\n",
        "  size = (width, height)\r\n",
        "  video = cv2.VideoWriter(video_path,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\r\n",
        "\r\n",
        "  for image in images:\r\n",
        "      video.write(cv2.imread(os.path.join(image_folder, image)))\r\n",
        "\r\n",
        "  cv2.destroyAllWindows()\r\n",
        "  video.release()\r\n",
        "\r\n",
        "\r\n",
        "video_path = '/content/drive/My Drive/CVS_videos/nn_indoor.avi'\r\n",
        "save_vid(image_folder, video_path, fps=24)\r\n",
        "\r\n",
        "# video_path = '/content/drive/My Drive/CVS_videos/nn_outdoor.avi'\r\n",
        "# save_vid(image_folder, video_path, fps=24)\r\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFj1z8DrE7w3"
      },
      "source": [
        "# **Incomplete**\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "## Improving inference using a BN (unkown environment)\r\n",
        "\r\n",
        "The idea here was to use previous frames to model predictions of the current frame. \r\n",
        "* We would assume that the neural network is correct about the environment on average every 24 frames (1 secs). We would therefore assume to know the previous environment (`T0`). This becomes evidence for our bayesian network.  \r\n",
        "* We would also assume the neural network is correct about the ***probabilistic predictions*** of objects in the video on average every 12 frames (0.5 secs). We therefore assume to know the probabilities of objects of the previous object (`S0`). \r\n",
        "* `T1` is the current environment and `S1` is the current obj.\r\n",
        "\r\n",
        "The aim is to obtain object probabilities of the current frame (`S1`) using the distribution described by the bayesian network. We will then use these probabilities to improve the probabilistic predictions of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aTBor6EzFVd"
      },
      "source": [
        "num_classes = len(class_to_idx)\r\n",
        "obj_dist = {c: 1/num_classes for c in class_to_idx.keys()}\r\n",
        "\r\n",
        "env = pd.DataFrame.from_dict({'indoor': [0.5], 'outdoor': [0.5]}) # P(env)\r\n",
        "obj = pd.DataFrame.from_dict({c: [1/num_classes] for c in class_to_idx.keys()}) # P(obj)\r\n",
        "env_env = csv_to_cond('/content/drive/My Drive/bayes_conditionals/env_env.csv') # P(current_env|prev_env)\r\n",
        "obj_env = csv_to_cond('/content/drive/My Drive/bayes_conditionals/obj_env.csv') # P(obj|prev_obj)\r\n",
        "obj_obj = csv_to_cond('/content/drive/My Drive/bayes_conditionals/obj_obj.csv') # P(obj|prev_obj, current_env)\r\n",
        "obj_obj_env = combine_cond(obj_obj, obj_env, obj)\r\n",
        "\r\n",
        "T0 = Node('T0', env)\r\n",
        "S0 = Node('S0', obj)\r\n",
        "T1 = Node('T1', env_env)\r\n",
        "S1 = Node('S0', obj_obj_env)\r\n",
        "\r\n",
        "e0 = (T0, T1)\r\n",
        "e1 = (T1, S0)\r\n",
        "e2 = (S0, S1)\r\n",
        "\r\n",
        "nodes = [T0, S0, T1, S1]\r\n",
        "edges = [e0, e1, e2]\r\n",
        "\r\n",
        "bn = BayesianNetwork(nodes, edges)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}